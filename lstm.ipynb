{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01924283-0022-4ffd-a6ad-eda20aa1b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import HTML\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from scipy.optimize import curve_fit\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from time import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccfdbca-305a-4321-bcc2-9dfc0b44bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57b3a2-ef81-4320-99cb-3da5bff6aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Datasets/recorridos-realizados-2018.csv', encoding='latin-1')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac5c713-b33a-41df-a51f-fc7ab46756f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir fecha_hora_retiro a datetime\n",
    "\n",
    "df.bici_Fecha_hora_retiro = df.bici_Fecha_hora_retiro.apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S'))\n",
    "df['date'] = df.bici_Fecha_hora_retiro.apply(lambda x: x.replace(minute=0, second=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b42e56-696f-4289-8b96-eb6c707cd9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpieza de datos\n",
    "\n",
    "fecha_limite = pd.to_datetime('2016-08-01 00:00:00')\n",
    "df_shorten = df[df['date'] >= fecha_limite].copy()\n",
    "bicis_por_dia = df_shorten.groupby('date').bici_id_usuario.count().resample('D').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc23f74-03f8-4f13-a0c3-240ee923a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtengo minutos de alquiler de bici\n",
    "\n",
    "parse_duration = lambda duration_str: timedelta(\n",
    "    hours=int(duration_str.split(':')[0]),\n",
    "    minutes=int(duration_str.split(':')[1]) if len(duration_str.split(':')) >=2 else 0,\n",
    "    seconds=int(duration_str.split(':')[2]) if len(duration_str.split(':')) >=3 else 0\n",
    ")\n",
    "\n",
    "df_shorten['bici_tiempo_uso_delta'] = df_shorten['bici_tiempo_uso'].apply(lambda x: parse_duration(x))\n",
    "df_shorten['total_minutes'] = df_shorten['bici_tiempo_uso_delta'].dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1433e4-3752-4fa5-8bb4-ac7e509b23af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trimmed = df_shorten[df_shorten['date'] <= pd.to_datetime('2017-12-01 23:59:59')].copy()\n",
    "bicis_por_dia_trimmed = df_trimmed.groupby('date').bici_id_usuario.count().resample('D').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4236b-1928-469a-b645-6148b1ddbfdb",
   "metadata": {},
   "source": [
    "### LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3d4ff-8ed4-441d-92d9-ee5e7e06de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bicis_por_dia_trimmed\n",
    "#size = int(len(X) * 0.66)\n",
    "#train, test = X[0:size], X[size:len(X)]\n",
    "X = pd.DataFrame(X)\n",
    "X = X.reset_index()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a753e-ff95-4644-bd9e-466dae77ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = X['bici_id_usuario'].values.astype('float32')\n",
    "plt.plot(timeseries)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "timeseries = timeseries.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20912a8-2070-41f2-9e46-7612937aef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split for time series\n",
    "train_size = int(len(timeseries) * 0.67)\n",
    "test_size = len(timeseries) - train_size\n",
    "train, test = timeseries[:train_size], timeseries[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712edd2-3677-49a8-8228-137484f9502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_dataset(dataset, lookback):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset)-lookback):\n",
    "        feature = dataset[i:i+lookback]  # Extract the feature window\n",
    "        target = dataset[i+1:i+lookback+1]  # Extract the target window (shifted by 1 step)\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    # Convert lists to PyTorch tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    # Reshape y to add an extra dimension\n",
    "    #y = y.unsqueeze(-1)\n",
    "    #X = X.unsqueeze(-1)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af0108-e3fa-45db-b3c8-786424827428",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968efe24-d239-4c4b-a829-670006a71076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#window\n",
    "\n",
    "lookback=14\n",
    "X_train, y_train = create_dataset(train, lookback=lookback)\n",
    "X_test, y_test = create_dataset(test, lookback=lookback)\n",
    "\n",
    "print(X_train.shape,  y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e3dbd3-f2b3-496d-9dcf-abb934c5b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BikePredModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=50, num_layers=1, batch_first=True)\n",
    "        self.linear = nn.Linear(50,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd50eea-dd8a-4698-80b1-5a3283209b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "model = BikePredModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.5)\n",
    "loss_fn = nn.MSELoss()\n",
    "#loss_fn = nn.HuberLoss()\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=64, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dca377-e8bf-47cc-893e-956d9bf3c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train = torch.tensor(scaler_x.fit_transform(X_train.squeeze().numpy())[:,:,None])\n",
    "X_test = torch.tensor(scaler_x.transform(X_test.squeeze().numpy())[:,:,None])\n",
    "\n",
    "# crear otro scaler\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train = torch.tensor(scaler_y.fit_transform(y_train.squeeze().numpy())[:,:,None])\n",
    "y_test = torch.tensor(scaler_y.transform(y_test.squeeze().numpy())[:,:,None])\n",
    "\n",
    "\n",
    "#X_test = torch.tensor(scaler.transform(X_test.squeeze().numpy())[:,:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df506b0-9e07-4500-8de6-2f8101bba6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "gradient_norm = []\n",
    "\n",
    "n_epochs = 3000\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 100 != 0:\n",
    "        continue\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_train = model(X_train)\n",
    "        train_loss.append(loss_fn(y_pred_train, y_train).item())\n",
    "        train_rmse.append(np.sqrt(loss_fn(y_pred_train, y_train).item()))\n",
    "\n",
    "        ## pruebo haciendo inverse transform\n",
    "        \n",
    "        \n",
    "        y_pred_test = model(X_test)\n",
    "        y_pred_test_inv = torch.tensor(scaler_y.inverse_transform(y_pred_test.squeeze().numpy())[:,:,None])\n",
    "        test_loss.append(loss_fn(y_pred_test_inv, y_test).item())\n",
    "        test_rmse.append(np.sqrt(loss_fn(y_pred_test_inv, y_test).item()))\n",
    "    \n",
    "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse[-1], test_rmse[-1]))\n",
    "    print(\"Epoch %d: train loss %.4f, test loss %.4f\" % (epoch, train_loss[-1], test_loss[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe0e77c-63b5-43b0-9bdf-379ec48ffb45",
   "metadata": {},
   "source": [
    "### Plot loss, rmse and gradient norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad24f0c-b71c-4716-b905-e2f4fafd8501",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(0, n_epochs, 100)\n",
    "\n",
    "plt.plot(epochs, train_loss, label='Train Loss')\n",
    "plt.plot(epochs, test_loss, label='Test Loss')\n",
    "\n",
    "plt.title('Training and Test Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a957fc-9e76-4760-85a7-4bf883409663",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # shift train predictions for plotting\n",
    "    train_plot = np.ones_like(timeseries)  * np.nan\n",
    "    y_pred = model(X_train)\n",
    "    y_pred = y_pred[:, -1, :]\n",
    "    train_plot[lookback:train_size] = model(X_train)[:, -1, :]\n",
    "  \n",
    "    # shift test predictions for plotting\n",
    "    test_plot = np.ones_like(timeseries) * np.nan\n",
    "    test_plot[train_size+lookback:len(timeseries)] = model(X_test)[:, -1, :]\n",
    "# plot\n",
    "plt.plot(timeseries, c='b')\n",
    "plt.plot(train_plot, c='r')\n",
    "plt.plot(test_plot, c='g')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obesity_risk",
   "language": "python",
   "name": "obesity_risk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
